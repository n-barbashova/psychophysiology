{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5deea49f",
   "metadata": {},
   "source": [
    "This code processes EDA and event-related data from .csv files generated by BIOPAC or similar devices. It structures and analyzes this data by applying conditions to identify events, downsampling the data, and organizing it for further analysis.  \n",
    "\n",
    "Digital Channels: BIOPAC uses a set of digital input channels (e.g., 8 channels, D0 to D7).\n",
    "Bit Position: \n",
    "D0 = 2^0 = 1  \n",
    "D1 = 2^1 = 2 \n",
    "D2 = 2^2 = 4 \n",
    "D3 = 2^3 = 8 \n",
    "D4 = 2^4 = 16 \n",
    "D5 = 2^5 = 32 \n",
    "D6 = 2^6 = 64 \n",
    "D7 = 2^7 = 128 \n",
    "\n",
    "Stim is in channel 16 (stim channel) \n",
    "GSR - EDA100C\t\n",
    "CORR - EMG100C\t\n",
    "ECG - ECG100C\t\n",
    "Feedback Cable - CBLCFMA - Current Feed\t\n",
    "Stim - Custom, AMI / HLT - A16\t\n",
    "Digital (STP Input 0)\t\n",
    "Digital (STP Input 1)\t\n",
    "Digital (STP Input 2)\t\n",
    "Digital (STP Input 3)\t\n",
    "Digital (STP Input 4)\t\n",
    "Digital (STP Input 5)\t\n",
    "Digital (STP Input 6)\t\n",
    "Digital (STP Input 7) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8bbb0",
   "metadata": {},
   "source": [
    "Event Code Notes (which digital channels): \n",
    "Flanker Stimulus appears: 128 (channel: 7)\n",
    "\n",
    "Each countdown/run starts with either: \n",
    "An event_code_countdown_start or event_code_flanker_start \n",
    "After the start of the flanker trial - there are these event codes: event_code_flanker \n",
    " \n",
    "countdown_end - this is when the whole coundfown section (60 seconds) ends and the electric shock is about to start. \n",
    "\n",
    "Note: errors from prev. script. keep just in case. \n",
    "In distal_shock condition: \n",
    "--- mistake countdown end: 91 (channel: 0, 1, 3, 6)\n",
    "prox stim \n",
    "------ mistake - - countdown end: 92 (channels: 2, 3, 6 )\n",
    "/end \n",
    "\n",
    "\n",
    "\n",
    "In distal_shock condition: \n",
    "countdown start: 81 (channel: 0, 4, 6 )\n",
    "countdown end: 91 (channel: 0, 1, 3, 4, 6) *** fixed \n",
    "flanker start: 4 (channel: 2)\n",
    "flanker end: 5 (channels: 0, 2)\n",
    "\n",
    "In proximal_shock condition:\n",
    "countdown start: 82 (channel: 1, 4, 6 )\n",
    "countdown end: 92 (channels: 2, 3, 4, 6 ) *** fixed \n",
    "flanker start: 8 (channel: 3)\n",
    "flanker end: 9(channels: 0, 3)\n",
    "\n",
    "In distal_light_stim condition:\n",
    "countdown start: 84 (channels: 2, 4, 6)\n",
    "countdown end: 94 (channels: 0, 1, 2, 3, 4, 6 )\n",
    "flanker start: 32 (channel: 5) \n",
    "flanker end: 33 (channels: 0, 5) \n",
    "\t\t \t\n",
    "In proximal_light_stim condition: \n",
    "countdown start: 85\t(channels: 0, 2, 4, 6) \n",
    "countdown end: 95 (channels: 0, 1, 2, 3, 4, 6)\n",
    "flanker start: 64 (channel 6) \n",
    "flanker end: 65 (channels: 0, 6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae0c11",
   "metadata": {},
   "source": [
    "- This code runs through each subject, then through each task file from that subject.\n",
    "- It imports the raw data of the task file (but aleady be in csv format after running an acq_to_mat script). \n",
    "- Each columns is a biopac channel (analog and digital channel).  \n",
    "- Each row represents a time point, with values captured based on the sampling rate (e.g., 2000 Hz = 2000 rows per second).\n",
    "- When an event code appears that event code tiggers a quick blip in the digital channels (either one or a combination of digital channels (cheat sheet above)). \n",
    "- To extract specific parts of the block - find the rows that have specific event codes. \n",
    "- Then break the data into 4 countdowns (there were 4 countdowns per block)  \n",
    "- Label those parts of the block \n",
    "- Downsample (every 20th row) to make it a lower temporal resolution \n",
    "- Repeat with each block \n",
    "- Concatenate all the selected parts of the data into one file \n",
    "- Export as text file \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e9eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    blockNum  intervalNum time_condition  shock_cond\n",
      "0          1            1       proximal       shock\n",
      "1          1            2         distal       shock\n",
      "2          1            3       proximal  light_stim\n",
      "3          1            4         distal  light_stim\n",
      "4          2            5       proximal  light_stim\n",
      "5          2            6       proximal       shock\n",
      "6          2            7         distal  light_stim\n",
      "7          2            8         distal       shock\n",
      "8          3            9       proximal       shock\n",
      "9          3           10         distal       shock\n",
      "10         3           11         distal  light_stim\n",
      "11         3           12       proximal  light_stim\n",
      "12         4           13       proximal  light_stim\n",
      "13         4           14         distal  light_stim\n",
      "14         4           15       proximal       shock\n",
      "15         4           16         distal       shock\n",
      "16         5           17       proximal  light_stim\n",
      "17         5           18         distal       shock\n",
      "18         5           19         distal  light_stim\n",
      "19         5           20       proximal       shock\n",
      "20         6           21         distal       shock\n",
      "21         6           22       proximal  light_stim\n",
      "22         6           23         distal  light_stim\n",
      "23         6           24       proximal       shock\n",
      "24         7           25       proximal       shock\n",
      "25         7           26         distal  light_stim\n",
      "26         7           27         distal       shock\n",
      "27         7           28       proximal  light_stim\n",
      "28         8           29       proximal  light_stim\n",
      "29         8           30         distal       shock\n",
      "30         8           31         distal  light_stim\n",
      "31         8           32       proximal       shock\n",
      "32         9           33         distal       shock\n",
      "33         9           34         distal  light_stim\n",
      "34         9           35       proximal       shock\n",
      "35         9           36       proximal  light_stim\n"
     ]
    }
   ],
   "source": [
    "# List of conditions for sub 3 (36 countdowns) - info to double check if this script matches up \n",
    "# import csv \n",
    "\n",
    "import pandas as pd\n",
    "file_path = \"/Users/nadezhdabarbashova/Library/CloudStorage/Dropbox/LEAP_Neuro_Lab/researchProjects/nadu/fmcc/pipeline_psychopyz_nadu/sub_3_conditions.csv\"\n",
    "df = pd.read_csv(file_path, index_col=None)\n",
    "df.reset_index(drop=True, inplace=True)  # Reset the index and remove the old one\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2919b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29567b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for collapsing across digital channels\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# event code cheat sheet from Jingyi\n",
    "# https://docs.google.com/spreadsheets/d/1-uQ2zdouvifLla07VZ8rfu7-LTdu7vqy/edit?gid=211668051#gid=211668051 \n",
    "\n",
    "# check = pd.read_csv('/Users/jingyiwang/Desktop/EB_modified/EB_psychopyz/ivn07_16.txt', delimiter='\\t')\n",
    "\n",
    "# change these \n",
    "save_dir = \"/Users/nadezhdabarbashova/Library/CloudStorage/Dropbox/LEAP_Neuro_Lab/researchProjects/nadu/fmcc/data/acq_data/EDA_processed\"\n",
    "rawdata = \"/Users/nadezhdabarbashova/Library/CloudStorage/Dropbox/LEAP_Neuro_Lab/researchProjects/nadu/fmcc/data/acq_data/fmcc_csv\"\n",
    "IDs = [\"03\"]\n",
    "ID = \"03\"\n",
    "\n",
    "#We need to record the start event code conditions for runs so that it can be used for the other analysis\n",
    "subject = []\n",
    "runli = []\n",
    "startcode = []\n",
    "run = \"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aedfe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.join(rawdata, ID)\n",
    "current_file = \"fmcc_sub\" + ID + \"_task_000\" + str(run) + \".csv\"\n",
    "path = os.path.join(current_dir, current_file)\n",
    "\n",
    "# Read the data file\n",
    "data = pd.read_csv(path, header=None, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32657a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.960018</td>\n",
       "      <td>-0.015259</td>\n",
       "      <td>0.377197</td>\n",
       "      <td>0.302124</td>\n",
       "      <td>-0.202938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.960018</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.381470</td>\n",
       "      <td>0.323486</td>\n",
       "      <td>-0.206295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.958492</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>0.386353</td>\n",
       "      <td>0.308228</td>\n",
       "      <td>-0.206600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.958492</td>\n",
       "      <td>-0.003052</td>\n",
       "      <td>0.390015</td>\n",
       "      <td>0.308228</td>\n",
       "      <td>-0.202938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586930</th>\n",
       "      <td>3.729061</td>\n",
       "      <td>-0.039673</td>\n",
       "      <td>-0.309753</td>\n",
       "      <td>0.283813</td>\n",
       "      <td>-0.207515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586931</th>\n",
       "      <td>3.727535</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>-0.308533</td>\n",
       "      <td>0.305176</td>\n",
       "      <td>-0.205074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586932</th>\n",
       "      <td>3.727535</td>\n",
       "      <td>-0.028992</td>\n",
       "      <td>-0.307007</td>\n",
       "      <td>0.305176</td>\n",
       "      <td>-0.201717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586933</th>\n",
       "      <td>3.729061</td>\n",
       "      <td>-0.041199</td>\n",
       "      <td>-0.307007</td>\n",
       "      <td>0.277710</td>\n",
       "      <td>-0.204464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586934</th>\n",
       "      <td>3.726010</td>\n",
       "      <td>-0.033569</td>\n",
       "      <td>-0.306091</td>\n",
       "      <td>0.299072</td>\n",
       "      <td>-0.203853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586935 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4    5    6    7    8   \\\n",
       "0       1.000000  2.000000  3.000000  4.000000  5.000000  6.0  7.0  8.0  9.0   \n",
       "1       2.960018 -0.015259  0.377197  0.302124 -0.202938  0.0  0.0  0.0  0.0   \n",
       "2       2.960018  0.006104  0.381470  0.323486 -0.206295  0.0  0.0  0.0  0.0   \n",
       "3       2.958492 -0.003052  0.386353  0.308228 -0.206600  0.0  0.0  0.0  0.0   \n",
       "4       2.958492 -0.003052  0.390015  0.308228 -0.202938  0.0  0.0  0.0  0.0   \n",
       "...          ...       ...       ...       ...       ...  ...  ...  ...  ...   \n",
       "586930  3.729061 -0.039673 -0.309753  0.283813 -0.207515  0.0  0.0  0.0  0.0   \n",
       "586931  3.727535 -0.019836 -0.308533  0.305176 -0.205074  0.0  0.0  0.0  0.0   \n",
       "586932  3.727535 -0.028992 -0.307007  0.305176 -0.201717  0.0  0.0  0.0  0.0   \n",
       "586933  3.729061 -0.041199 -0.307007  0.277710 -0.204464  0.0  0.0  0.0  0.0   \n",
       "586934  3.726010 -0.033569 -0.306091  0.299072 -0.203853  0.0  0.0  0.0  0.0   \n",
       "\n",
       "          9     10    11    12  \n",
       "0       10.0  11.0  12.0  13.0  \n",
       "1        0.0   0.0   0.0   0.0  \n",
       "2        0.0   0.0   0.0   0.0  \n",
       "3        0.0   0.0   0.0   0.0  \n",
       "4        0.0   0.0   0.0   0.0  \n",
       "...      ...   ...   ...   ...  \n",
       "586930   0.0   0.0   0.0   0.0  \n",
       "586931   0.0   0.0   0.0   0.0  \n",
       "586932   0.0   0.0   0.0   0.0  \n",
       "586933   0.0   0.0   0.0   0.0  \n",
       "586934   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[586935 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a1c2b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conditions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#then I add a new column to our data frame based on these conditions\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m data_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mselect(conditions, values)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m#select useful columns\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m#If you need the other data: e.g. EMG then change the name here.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m data_1_save \u001b[38;5;241m=\u001b[39m data_1[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEDA\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conditions' is not defined"
     ]
    }
   ],
   "source": [
    "# Find the countdown start EC \n",
    "# Find an elevation in channel 16 \n",
    "# Find flanker start / flanker end in those rows \n",
    "# Then downsample\n",
    "# export as txt \n",
    "# Next DF should not have this part. \n",
    "\n",
    "# Find next countdown start EC, repeat. \n",
    "\n",
    "# Process the data for each ID and run\n",
    "for ID in IDs:\n",
    "    for run in range(9): # 8 runs (go from 0 to 8) - put 9 as the limit  \n",
    "        \n",
    "        #this is an empty list - when we start processing it, the subject gets added to the list \n",
    "        subject.append(ID)\n",
    "        \n",
    "        #empty list of runs - add run to the list \n",
    "        runli.append(run+1)\n",
    "        current_dir = rawdata + \"/\" + ID\n",
    "        \n",
    "        # fmcc_sub03_task_0000.csv  - get the file path \n",
    "        current_file = \"fmcc_sub\" + ID + \"_task_000\" + str(run) + \".csv\"\n",
    "        path = os.path.join(current_dir, current_file)\n",
    "        \n",
    "        # csv file from acqknowledge - make it a df \n",
    "        tmp_df = pd.read_csv(path, header=None, delimiter=',')  \n",
    "        data=tmp_df #to make a copy just in case\n",
    "                \n",
    "        # make sure this is accurate to my channel names - in previous step check headers - can also check this on acqknolwedge \n",
    "        # EDA, CORR, ECG, Feedback Cable - CBLCFMA - Current Feed\tStim - Custom, AMI / HLT - A16\tDigital (STP Input 0)\tDigital (STP Input 1)\tDigital (STP Input 2)\tDigital (STP Input 3)\tDigital (STP Input 4)\tDigital (STP Input 5)\tDigital (STP Input 6)\tDigital (STP Input 7)\n",
    "        # it starts from channel 0 \n",
    "        data.columns = ['EDA', 'Corr', 'ECGmV', 'feedback', 'stim', 'ch0', 'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7']\n",
    "        \n",
    "        # Create new columns defining condition and timing\n",
    "         # Create new columns defining condition and timing\n",
    "        condition_conditions = [\n",
    "            # Distal shock conditions\n",
    "            ((data['ch0'] == 5) & (data['ch4'] == 5) & (data['ch6'] == 5) & \n",
    "             (data['ch1'] == 0) & (data['ch2'] == 0) & (data['ch3'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # distal shock countdown start\n",
    "            ((data['ch0'] == 5) & (data['ch1'] == 5) & (data['ch3'] == 5) & \n",
    "             (data['ch4'] == 5) & (data['ch6'] == 5) & (data['ch2'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # distal shock countdown end\n",
    "            ((data['ch2'] == 5) & (data['ch0'] == 0) & (data['ch1'] == 0) & \n",
    "             (data['ch3'] == 0) & (data['ch4'] == 0) & (data['ch5'] == 0) & \n",
    "             (data['ch6'] == 0) & (data['ch7'] == 0)),  # distal shock flanker start\n",
    "            ((data['ch2'] == 5) & (data['ch0'] == 5) & (data['ch1'] == 0) & \n",
    "             (data['ch3'] == 0) & (data['ch4'] == 0) & (data['ch5'] == 0) & \n",
    "             (data['ch6'] == 0) & (data['ch7'] == 0)),  # distal shock flanker end\n",
    "\n",
    "            # Proximal shock conditions\n",
    "            ((data['ch1'] == 5) & (data['ch4'] == 5) & (data['ch6'] == 5) & \n",
    "             (data['ch0'] == 0) & (data['ch2'] == 0) & (data['ch3'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # proximal shock countdown start\n",
    "            ((data['ch2'] == 5) & (data['ch3'] == 5) & (data['ch4'] == 5) & \n",
    "             (data['ch6'] == 5) & (data['ch0'] == 0) & (data['ch1'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # proximal shock countdown end\n",
    "            ((data['ch3'] == 5) & (data['ch0'] == 0) & (data['ch1'] == 0) & \n",
    "             (data['ch2'] == 0) & (data['ch4'] == 0) & (data['ch5'] == 0) & \n",
    "             (data['ch6'] == 0) & (data['ch7'] == 0)),  # proximal shock flanker start\n",
    "            ((data['ch3'] == 5) & (data['ch0'] == 5) & (data['ch1'] == 0) & \n",
    "             (data['ch2'] == 0) & (data['ch4'] == 0) & (data['ch5'] == 0) & \n",
    "             (data['ch6'] == 0) & (data['ch7'] == 0)),  # proximal shock flanker end\n",
    "\n",
    "            # Distal stim conditions\n",
    "            ((data['ch2'] == 5) & (data['ch4'] == 5) & (data['ch6'] == 5) & \n",
    "             (data['ch0'] == 0) & (data['ch1'] == 0) & (data['ch3'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # distal stim countdown start\n",
    "            ((data['ch0'] == 5) & (data['ch1'] == 5) & (data['ch2'] == 5) & \n",
    "             (data['ch3'] == 5) & (data['ch4'] == 5) & (data['ch6'] == 5) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # distal stim countdown end\n",
    "            ((data['ch5'] == 5) & (data['ch0'] == 0) & (data['ch1'] == 0) & \n",
    "             (data['ch2'] == 0) & (data['ch3'] == 0) & (data['ch4'] == 0) & \n",
    "             (data['ch6'] == 0) & (data['ch7'] == 0)),  # distal stim flanker start\n",
    "            ((data['ch5'] == 5) & (data['ch0'] == 5) & (data['ch1'] == 0) & \n",
    "             (data['ch2'] == 0) & (data['ch3'] == 0) & (data['ch4'] == 0) & \n",
    "             (data['ch6'] == 0) & (data['ch7'] == 0)),  # distal stim flanker end\n",
    "\n",
    "            # Proximal stim conditions\n",
    "            ((data['ch0'] == 5) & (data['ch2'] == 5) & (data['ch4'] == 5) & \n",
    "             (data['ch6'] == 5) & (data['ch1'] == 0) & (data['ch3'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # proximal stim countdown start\n",
    "            ((data['ch0'] == 5) & (data['ch1'] == 5) & (data['ch2'] == 5) & \n",
    "             (data['ch3'] == 5) & (data['ch4'] == 5) & (data['ch6'] == 5) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # proximal stim countdown end\n",
    "            ((data['ch6'] == 5) & (data['ch0'] == 0) & (data['ch1'] == 0) & \n",
    "             (data['ch2'] == 0) & (data['ch3'] == 0) & (data['ch4'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0)),  # proximal stim flanker start\n",
    "            ((data['ch6'] == 5) & (data['ch0'] == 5) & (data['ch1'] == 0) & \n",
    "             (data['ch2'] == 0) & (data['ch3'] == 0) & (data['ch4'] == 0) & \n",
    "             (data['ch5'] == 0) & (data['ch7'] == 0))   # proximal stim flanker end\n",
    "        ]\n",
    "        \n",
    "        condition_values = [\n",
    "            'distal shock', 'distal shock', 'distal shock', 'distal shock',\n",
    "            'proximal shock', 'proximal shock', 'proximal shock', 'proximal shock',\n",
    "            'distal stim', 'distal stim', 'distal stim', 'distal stim',\n",
    "            'proximal stim', 'proximal stim', 'proximal stim', 'proximal stim'\n",
    "        ]\n",
    "\n",
    "        # create a new col \"Condition\" in df Data - Assign condition labels\n",
    "        data['Condition'] = np.select(condition_conditions, condition_values, default='none')\n",
    "                \n",
    "        # create a name and file patth to export this labelled data as a csv \n",
    "        test_label_file = os.path.join(save_dir, f\"test_label_{ID}_run{run + 1}.csv\")\n",
    "        \n",
    "        # export as csv - check it against the other csv (from psychopy)  \n",
    "        data.to_csv(test_label_file, index=False)\n",
    "\n",
    "        \n",
    "        #This list is how we decode the values of the event channels, and this corresponds to the conditions list\n",
    "        #Make sure you have the same # of elements in the conditions and values lists\n",
    "        # values = ['Neg', '0', '1', '2', '3', '4', '5', '6', 'start']\n",
    "        values = [7, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "        #then I add a new column to our data frame based on these conditions\n",
    "        data_1['Event'] = np.select(conditions, values)\n",
    "\n",
    "\n",
    "        #select useful columns\n",
    "        #If you need the other data: e.g. EMG then change the name here.\n",
    "        data_1_save = data_1[['EDA',\"Event\"]]\n",
    "\n",
    "        savefile = ID + \"_encoding_\" + str(run+1) + \".txt\"\n",
    "        \n",
    "        #downsample\n",
    "       \n",
    "        # Select every 20th row\n",
    "        encoding_downsample = data_1_save[::20]\n",
    "        encoding_downsample.reset_index(inplace=True, drop=True)\n",
    "        # add a column for time points.\n",
    "        \n",
    "        #The number \"0.01\" here dependents on your sampling rate. I have 2000 sampling rate, then I downsampled 20 folds, so now it is 100 sampling rate for the \"encoding_downsample\". In this case, I use 1/100 = 0.01\n",
    "        encoding_downsample['timepoint'] = 0.01 * encoding_downsample.index\n",
    "       \n",
    "        #reorder columns\n",
    "        encoding_downsample = encoding_downsample[['timepoint','EDA','Event']]\n",
    "        # remove event code when appeared twice in the same trial\n",
    "        indexli = []\n",
    "        \n",
    "        \n",
    "        for k in range(len(encoding_downsample)):\n",
    "            if k < len(encoding_downsample) - 1:\n",
    "                Code1 = encoding_downsample.loc[k]['Event']\n",
    "                Code2 = encoding_downsample.loc[k + 1]['Event']\n",
    "                if int(Code1) > 0 and int(Code2) > 0:\n",
    "                    indexli.append(k + 1)\n",
    "        for ind in indexli:\n",
    "            encoding_downsample.loc[ind, \"Event\"] = 0\n",
    "        \n",
    "        save = os.path.join(save_dir, savefile)\n",
    "        encoding_downsample.to_csv(save, header=None, index=None, sep='\\t', mode='a')\n",
    "        \n",
    "\n",
    "        savefile = ID + \"_task_\" + str(run+1) + \".txt\"\n",
    "        save = os.path.join(save_dir, savefile)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63361afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####old\n",
    "# find first row that has countdown start or flanker start \n",
    "# start a df using this index as start, and shock as end. \n",
    "# label rows in between.  \n",
    "# then repeat this but with new df - until all 4 are captured. \n",
    "\n",
    "\n",
    "# Define a function that find the index of \"not none\" elements in the list\n",
    "def find_non_none_indices(lst):\n",
    "    indices = [i for i, element in enumerate(lst) if element != 'none']\n",
    "    return indices\n",
    "\n",
    "\n",
    "for ID in IDs:\n",
    "    for run in range(9): # 8 runs (go from 0 to 8) - put 9 as the limit  \n",
    "        \n",
    "        #this is an empty list - when we start processing it, the subject gets added to the list \n",
    "        subject.append(ID)\n",
    "        \n",
    "        #empty list of runs - add run to the list \n",
    "        runli.append(run+1)\n",
    "        current_dir = rawdata + \"/\" + ID\n",
    "        \n",
    "        # fmcc_sub03_task_0000.csv \n",
    "        current_file = \"fmcc_sub\" + ID + \"_task_000\" + str(run) + \".csv\"\n",
    "        path = os.path.join(current_dir, current_file)\n",
    "        \n",
    "        # csv file from acqknowledge - make it a df \n",
    "        tmp_df = pd.read_csv(path, header=None, delimiter=',') \n",
    "\n",
    "        #there is probably a much more sophisticated way to do this, but I check the value of every column for the digital channels\n",
    "        #check your txt file - I always clean the header and filter out any extra details until I only have the time & channels with the data I want\n",
    "\n",
    "        data=tmp_df #to make a copy just in case\n",
    "                \n",
    "        # make sure this is accurate to my channel names - in previous step check headers - can also check this on acqknolwedge \n",
    "        # EDA, CORR, ECG, Feedback Cable - CBLCFMA - Current Feed\tStim - Custom, AMI / HLT - A16\tDigital (STP Input 0)\tDigital (STP Input 1)\tDigital (STP Input 2)\tDigital (STP Input 3)\tDigital (STP Input 4)\tDigital (STP Input 5)\tDigital (STP Input 6)\tDigital (STP Input 7)\n",
    "        # it starts from channel 0 \n",
    "        data.columns = ['EDA', 'Corr', 'ECGmV', 'feedback', 'stim', 'ch0', 'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7']\n",
    "        \n",
    "        #Count how many start event code is caught, sometimes some runs only have 1 start event code caught.\n",
    "        #first get the start_li and downsample\n",
    "                \n",
    "        # Change this - first row where countdown - that is the start \n",
    "        \n",
    "        # make sure all 4 possible countdown starts are here \n",
    "        # create keys and values - any of those condition - that means start \n",
    "       \n",
    "        conditions = [#eventcode=221 => start of block\n",
    "        (data['ch1'] == 5) & (data['ch2'] == 5) & (data['ch3'] == 0) & (data['ch4'] == 0) & (data['ch5'] == 5) & (data['ch6'] == 0) & (data['ch7'] == 5) & (data['ch8'] == 5)]\n",
    "        #values\n",
    "        values = [\"start\"]\n",
    "        \n",
    "        # then I add a new start column to the original data frame to put the start point\n",
    "        # put start wherever those conditions are true (the cols have the values above)\n",
    "        data['start'] = np.select(conditions, values, default=\"none\")\n",
    "        \n",
    "        # turn the start column from data df into a list - make the list shorter \n",
    "        Start_li = data['start'].tolist()[::10]\n",
    "        Start_num = find_non_none_indices (Start_li)\n",
    "        startcode.append(len(Start_num))\n",
    "        \n",
    "        if len(Start_num) == 2:\n",
    "            #Get the first encoding session\n",
    "            # here - remove everything before index - save everything starting from index to the end \n",
    "            data_tmp = data.iloc[index:]\n",
    "            \n",
    "            #reset index\n",
    "            data_tmp.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            # find a way to cut each run into coundowns -  shock multiplied by sampling rate \n",
    "            index1 = data_tmp['start'].tolist().index('start')\n",
    "            index2 = index1+20\n",
    "            data_1 = data_tmp.iloc[:index1]\n",
    "            \n",
    "            # reset index\n",
    "            data_1.reset_index(inplace=True, drop=True)\n",
    "            data_2 = data_tmp.iloc[index2:]\n",
    "            \n",
    "            # reset index \n",
    "            data_2.reset_index(inplace=True, drop=True)\n",
    "            \n",
    "            #then I check the value in every digital channel column and make a 'conditions' list. Then I make a list of the events associated with those conditions\n",
    "            #Event code conditions: see EventCode_Cheetsheet for details.\n",
    "            conditions = [\n",
    "                \n",
    "            # change this for FMCC - check the psychopy script \n",
    "            # event code - flanker task \n",
    "            # event code - countdown \n",
    "                 \n",
    "            #So for example, when ch1 is on it is negative image.\n",
    "            # (data_1['ch1'] == 5) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "            #When ch2 is on it is first position, no change = 0\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 5) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "\n",
    "            #When ch3 is on it is color change-within emotion = 1\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 5) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "            #When ch4 is on it is color change-within neutral = 2\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 5) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "            #When ch5 is on it is emotional->neutral change = 3\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 5) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "            #When ch6 is on it is neutral->emotional change = 4\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 5) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "            #When ch7 is on it is emotional->neutral change + color = 5\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 5) & (data_1['ch8'] == 0),\n",
    "            #When ch8 is on it is neutral->emotional change + color = 6\n",
    "            (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 5)\n",
    "            ]\n",
    "            #This list is how we decode the values of the event channels, and this corresponds to the conditions list\n",
    "            #Make sure you have the same # of elements in the conditions and values lists\n",
    "            # values = ['Neg', '0', '1', '2', '3', '4', '5', '6', 'start']\n",
    "            \n",
    "            # this is something I name myself - what each number represents \n",
    "            values = [7, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "            #then I add a new column to our data frame based on these conditions\n",
    "            data_1['Event'] = np.select(conditions, values)\n",
    "\n",
    "            #do the same thing for data_2\n",
    "            # Event code conditions: see EventCode_Cheetsheet for details.\n",
    "            \n",
    "            conditions = [\n",
    "                # So for example, when ch1 is on it is negative image.\n",
    "                # (data_2['ch1'] == 5) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                #             data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch2 is on it is first position, no change = 0\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 5) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                            data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "\n",
    "                # When ch3 is on it is color change-within emotion = 1\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 5) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                            data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch4 is on it is color change-within neutral = 2\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 5) & (data_2['ch5'] == 0) & (\n",
    "                            data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch5 is on it is emotional->neutral change = 3\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 5) & (\n",
    "                            data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch6 is on it is neutral->emotional change = 4\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                            data_2['ch6'] == 5) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch7 is on it is emotional->neutral change + color = 5\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                            data_2['ch6'] == 0) & (data_2['ch7'] == 5) & (data_2['ch8'] == 0),\n",
    "                # When ch8 is on it is neutral->emotional change + color = 6\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                            data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 5)\n",
    "            ]\n",
    "           \n",
    "        # This list is how we decode the values of the event channels, and this corresponds to the conditions list\n",
    "            # Make sure you have the same # of elements in the conditions and values lists\n",
    "            # values = ['Neg', '0', '1', '2', '3', '4', '5', '6', 'start']\n",
    "            values = [7, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "            # then I add a new column to our data frame based on these conditions\n",
    "            data_2['Event'] = np.select(conditions, values)\n",
    "\n",
    "            #Break data_1 and data_1 into encoding and task phase\n",
    "            #First get the code that appeared in the last\n",
    "            lastcode = data_1['Event'].unique()[4]\n",
    "            \n",
    "            #The lastcode is the last image, we need to add the total time of that trial (6.5s) with sample rate as 2000\n",
    "            endindex = data_1['Event'][::-1].tolist().index(lastcode)\n",
    "            endindex = len(data_1) - 1 - endindex\n",
    "            endindex1 = int(endindex + 6.5*2000)\n",
    "\n",
    "            #Start of task, basically the end of encoding plus the 45s distraction\n",
    "            taskstart = int(endindex1 + 45*2000)\n",
    "\n",
    "            data_1_encoding = data_1[:endindex1]\n",
    "            data_1_encoding.reset_index(inplace=True, drop=True)\n",
    "            data_1_task = data_1[taskstart:]\n",
    "            data_1_task.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            # Do the same thing for data_2\n",
    "            # First get the code that appeared in the last\n",
    "            lastcode = data_2['Event'].unique()[4]\n",
    "            # The lastcode is the last image, we need to add the total time of that trial (6.5s) with sample rate as 2000\n",
    "            endindex = data_2['Event'][::-1].tolist().index(lastcode)\n",
    "            endindex = len(data_2) - 1 - endindex\n",
    "            endindex2 = int(endindex + 6.5 * 2000)\n",
    "\n",
    "            # Start of task, basically the end of encoding plus the 45s distraction\n",
    "            taskstart = int(endindex2 + 45 * 2000)\n",
    "\n",
    "            data_2_encoding = data_2[:endindex2]\n",
    "            data_2_encoding.reset_index(inplace=True, drop=True)\n",
    "            data_2_task = data_2[taskstart:]\n",
    "            data_2_task.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            #select useful columns\n",
    "            data_1_encoding_save = data_1_encoding[['EDA',\"Event\"]]\n",
    "\n",
    "            data_1_task_save = data_1_task[['EDA','Event']]\n",
    "            data_2_encoding_save = data_2_encoding[['EDA', \"Event\"]]\n",
    "            data_2_task_save = data_2_task[['EDA', 'Event']]\n",
    "\n",
    "            #Merge the two part of encoding\n",
    "            encoding = pd.concat([data_1_encoding_save, data_2_encoding_save], ignore_index=True)\n",
    "            task = pd.concat([data_1_task_save, data_2_task_save], ignore_index=True)\n",
    "            \n",
    "            # we save it as a txt file \n",
    "            savefile = ID + \"_encoding_\" + str(run+1) + \".txt\"\n",
    "            #downsample\n",
    "            \n",
    "            # Select every 20th row\n",
    "            # put the time stamp back in  - Leda Lab like downsampling \n",
    "            encoding_downsample = encoding[::20]\n",
    "            encoding_downsample.reset_index(inplace=True, drop=True)\n",
    "            \n",
    "            # add a column for time points \n",
    "            encoding_downsample['timepoint'] = 0.01 * encoding_downsample.index\n",
    "            \n",
    "            # reorder columns \n",
    "            encoding_downsample = encoding_downsample[['timepoint','EDA','Event']]\n",
    "            \n",
    "                \n",
    "            # this will save as txt file - for Leda Lab  \n",
    "            encoding_downsample.to_csv(save, header=None, index=None, sep='\\t', mode='a')\n",
    "\n",
    "            savefile = ID + \"_task_\" + str(run+1) + \".txt\"\n",
    "            save = os.path.join(save_dir, savefile)\n",
    "\n",
    "            # downsample\n",
    "            # Select every 20th row\n",
    "            task_downsample = task[::20]\n",
    "            task_downsample.reset_index(inplace=True, drop=True)\n",
    "            \n",
    "            # add a column for time points\n",
    "            task_downsample['timepoint'] = 0.01 * task_downsample.index\n",
    "            # reorder columns\n",
    "            task_downsample = task_downsample[['timepoint', 'EDA', 'Event']]\n",
    "            \n",
    "            # remove event code when appeared twice in the same trial\n",
    "            indexli = []\n",
    "            for k in range(len(task_downsample)):\n",
    "                if k < len(task_downsample) - 1:\n",
    "                    Code1 = task_downsample.loc[k]['Event']\n",
    "                    Code2 = task_downsample.loc[k + 1]['Event']\n",
    "                    if int(Code1) > 0 and int(Code2) > 0:\n",
    "                        indexli.append(k + 1)\n",
    "            for ind in indexli:\n",
    "                task_downsample.loc[ind, \"Event\"] = 0\n",
    "\n",
    "            task_downsample.to_csv(save, header=None, index=None, sep='\\t', mode='a')\n",
    "        \n",
    "        if len(Start_num) == 1:\n",
    "            \n",
    "            # Get the first encoding session\n",
    "            # make two parts (within a sequence we have two parts)\n",
    "            data_tmp = data\n",
    "            index1 = data_tmp['start'].tolist().index('start')\n",
    "            index2 = index1 + 20\n",
    "\n",
    "            data_1 = data_tmp.iloc[:index1]\n",
    "            # reset index\n",
    "            data_1.reset_index(inplace=True, drop=True)\n",
    "            data_2 = data_tmp.iloc[index2:]\n",
    "            # reset index\n",
    "            data_2.reset_index(inplace=True, drop=True)\n",
    "            # then I check the value in every digital channel column and make a 'conditions' list. Then I make a list of the events associated with those conditions\n",
    "            \n",
    "            # Event code conditions: see EventCode_Cheetsheet for details.\n",
    "            conditions = [\n",
    "                # So for example, when ch1 is on it is negative image.\n",
    "                # (data_1['ch1'] == 5) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "                \n",
    "                # When ch2 is on it is first position, no change = 0\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 5) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (\n",
    "                            data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "\n",
    "                # When ch3 is on it is color change-within emotion = 1\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 5) & (data_1['ch4'] == 0) & (\n",
    "                            data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "                # When ch4 is on it is color change-within neutral = 2\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 5) & (\n",
    "                            data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "                # When ch5 is on it is emotional->neutral change = 3\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (\n",
    "                            data_1['ch5'] == 5) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "                # When ch6 is on it is neutral->emotional change = 4\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (\n",
    "                            data_1['ch5'] == 0) & (data_1['ch6'] == 5) & (data_1['ch7'] == 0) & (data_1['ch8'] == 0),\n",
    "                # When ch7 is on it is emotional->neutral change + color = 5\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (\n",
    "                            data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 5) & (data_1['ch8'] == 0),\n",
    "                # When ch8 is on it is neutral->emotional change + color = 6\n",
    "                (data_1['ch1'] == 0) & (data_1['ch2'] == 0) & (data_1['ch3'] == 0) & (data_1['ch4'] == 0) & (\n",
    "                            data_1['ch5'] == 0) & (data_1['ch6'] == 0) & (data_1['ch7'] == 0) & (data_1['ch8'] == 5)\n",
    "            ]\n",
    "            # This list is how we decode the values of the event channels, and this corresponds to the conditions list\n",
    "            # Make sure you have the same # of elements in the conditions and values lists\n",
    "            # values = ['Neg', '0', '1', '2', '3', '4', '5', '6', 'start']\n",
    "            values = [7, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "            # then I add a new column to our data frame based on these conditions\n",
    "            data_1['Event'] = np.select(conditions, values)\n",
    "\n",
    "            # do the same thing for data_2\n",
    "            # Event code conditions: see EventCode_Cheetsheet for details.\n",
    "            conditions = [\n",
    "                # So for example, when ch1 is on it is negative image.\n",
    "                # (data_2['ch1'] == 5) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (data_2['ch5'] == 0) & (\n",
    "                #             data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch2 is on it is first position, no change = 0\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 5) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (\n",
    "                            data_2['ch5'] == 0) & (\n",
    "                        data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "\n",
    "                # When ch3 is on it is color change-within emotion = 1\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 5) & (data_2['ch4'] == 0) & (\n",
    "                            data_2['ch5'] == 0) & (\n",
    "                        data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch4 is on it is color change-within neutral = 2\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 5) & (\n",
    "                            data_2['ch5'] == 0) & (\n",
    "                        data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch5 is on it is emotional->neutral change = 3\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (\n",
    "                            data_2['ch5'] == 5) & (\n",
    "                        data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch6 is on it is neutral->emotional change = 4\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (\n",
    "                            data_2['ch5'] == 0) & (\n",
    "                        data_2['ch6'] == 5) & (data_2['ch7'] == 0) & (data_2['ch8'] == 0),\n",
    "                # When ch7 is on it is emotional->neutral change + color = 5\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (\n",
    "                            data_2['ch5'] == 0) & (\n",
    "                        data_2['ch6'] == 0) & (data_2['ch7'] == 5) & (data_2['ch8'] == 0),\n",
    "                # When ch8 is on it is neutral->emotional change + color = 6\n",
    "                (data_2['ch1'] == 0) & (data_2['ch2'] == 0) & (data_2['ch3'] == 0) & (data_2['ch4'] == 0) & (\n",
    "                            data_2['ch5'] == 0) & (\n",
    "                        data_2['ch6'] == 0) & (data_2['ch7'] == 0) & (data_2['ch8'] == 5)\n",
    "            ]\n",
    "            # This list is how we decode the values of the event channels, and this corresponds to the conditions list\n",
    "            # Make sure you have the same # of elements in the conditions and values lists\n",
    "            # values = ['Neg', '0', '1', '2', '3', '4', '5', '6', 'start']\n",
    "            values = [7, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "            # then I add a new column to our data frame based on these conditions\n",
    "            data_2['Event'] = np.select(conditions, values)\n",
    "\n",
    "            # Break data_1 and data_1 into encoding and task phase\n",
    "            # First get the code that appeared in the last\n",
    "            lastcode = data_1['Event'].unique()[4]\n",
    "            # The lastcode is the last image, we need to add the total time of that trial (6.5s) with sample rate as 2000\n",
    "            endindex = data_1['Event'][::-1].tolist().index(lastcode)\n",
    "            endindex = len(data_1) - 1 - endindex\n",
    "            endindex1 = int(endindex + 6.5 * 2000)\n",
    "\n",
    "            # Start of task, basically the end of encoding plus the 45s distraction\n",
    "            taskstart = int(endindex1 + 45 * 2000)\n",
    "\n",
    "            data_1_encoding = data_1[:endindex1]\n",
    "            data_1_encoding.reset_index(inplace=True, drop=True)\n",
    "            data_1_task = data_1[taskstart:]\n",
    "            data_1_task.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            # Do the same thing for data_2\n",
    "            # First get the code that appeared in the last\n",
    "            lastcode = data_2['Event'].unique()[4]\n",
    "            # The lastcode is the last image, we need to add the total time of that trial (6.5s) with sample rate as 2000\n",
    "            endindex = data_2['Event'][::-1].tolist().index(lastcode)\n",
    "            endindex = len(data_2) - 1 - endindex\n",
    "            endindex2 = int(endindex + 6.5 * 2000)\n",
    "\n",
    "            # Start of task, basically the end of encoding plus the 45s distraction\n",
    "            taskstart = int(endindex2 + 45 * 2000)\n",
    "\n",
    "            data_2_encoding = data_2[:endindex2]\n",
    "            data_2_encoding.reset_index(inplace=True, drop=True)\n",
    "            data_2_task = data_2[taskstart:]\n",
    "            data_2_task.reset_index(inplace=True, drop=True)\n",
    "\n",
    "            # select useful columns\n",
    "            data_1_encoding_save = data_1_encoding[['EDA', \"Event\"]]\n",
    "            data_1_task_save = data_1_task[['EDA', 'Event']]\n",
    "            data_2_encoding_save = data_2_encoding[['EDA', \"Event\"]]\n",
    "            data_2_task_save = data_2_task[['EDA', 'Event']]\n",
    "\n",
    "            # Merge the two part of encoding\n",
    "            encoding = pd.concat([data_1_encoding_save, data_2_encoding_save], ignore_index=True)\n",
    "            task = pd.concat([data_1_task_save, data_2_task_save], ignore_index=True)\n",
    "\n",
    "            savefile = ID + \"_encoding_\" + str(run + 1) + \".txt\"\n",
    "            # downsample\n",
    "            # Select every 20th row\n",
    "            encoding_downsample = encoding[::20]\n",
    "            encoding_downsample.reset_index(inplace=True, drop=True)\n",
    "            # add a column for time points\n",
    "            encoding_downsample['timepoint'] = 0.01 * encoding_downsample.index\n",
    "            # reorder columns\n",
    "            encoding_downsample = encoding_downsample[['timepoint', 'EDA', 'Event']]\n",
    "            # remove event code when appeared twice in the same trial\n",
    "            indexli = []\n",
    "            for k in range(len(encoding_downsample)):\n",
    "                if k < len(encoding_downsample) - 1:\n",
    "                    Code1 = encoding_downsample.loc[k]['Event']\n",
    "                    Code2 = encoding_downsample.loc[k + 1]['Event']\n",
    "                    if int(Code1) > 0 and int(Code2) > 0:\n",
    "                        indexli.append(k + 1)\n",
    "            for ind in indexli:\n",
    "                encoding_downsample.loc[ind, \"Event\"] = 0\n",
    "            save = os.path.join(save_dir, savefile)\n",
    "            encoding_downsample.to_csv(save, header=None, index=None, sep='\\t', mode='a')\n",
    "\n",
    "            savefile = ID + \"_task_\" + str(run + 1) + \".txt\"\n",
    "            save = os.path.join(save_dir, savefile)\n",
    "\n",
    "            # downsample\n",
    "            # Select every 20th row\n",
    "            task_downsample = task[::20]\n",
    "            task_downsample.reset_index(inplace=True, drop=True)\n",
    "            # add a column for time points\n",
    "            task_downsample['timepoint'] = 0.01 * task_downsample.index\n",
    "            # reorder columns\n",
    "            task_downsample = task_downsample[['timepoint', 'EDA', 'Event']]\n",
    "           \n",
    " \n",
    "            for ind in indexli:\n",
    "                task_downsample.loc[ind, \"Event\"] = 0\n",
    "\n",
    "            task_downsample.to_csv(save, header=None, index=None, sep='\\t', mode='a')\n",
    "\n",
    "#Make a dataframe and save the start event code info\n",
    "StartCodeSave1 = {\n",
    "    'ID': subject,\n",
    "    'Run': runli,\n",
    "    'StartCodeNum': startcode\n",
    "}\n",
    "\n",
    "\n",
    "StartCodeSave = pd.DataFrame(StartCodeSave1)\n",
    "StartCodeSave.to_csv(\"/Users/nadezhdabarbashova/Documents/fmcc/data/fmcc_csv/StartCodeStatus.csv\", index=False)\n",
    "\n",
    "#So now we will have a column filled with the elements in our 'values' lists based on the conditions we gave it in the 'conditions' list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "308858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.append(ID)\n",
    "run = 1\n",
    "current_dir = rawdata + \"/\" + ID\n",
    "sub = \"03\"\n",
    "# fmcc_sub03_task_0000.csv \n",
    "current_file = \"fmcc_sub\" + ID + \"_task_000\" + str(run) + \".csv\"\n",
    "path = os.path.join(current_dir, current_file)\n",
    "tmp_df = pd.read_csv(path, header=None, delimiter=',') #your txt or csv file from acqknowledge\n",
    "\n",
    "#there is probably a much more sophisticated way to do this, but I check the value of every column for the digital channels\n",
    "#check your txt file - I always clean the header and filter out any extra details until I only have the time & channels with the data I want\n",
    "\n",
    "data=tmp_df #to make a copy just in case\n",
    "        \n",
    "#then I rename the channels just slightly because they always seem to be a bit 'off'\n",
    "        \n",
    "# make sure this is accurate to my channel names - in previous step check headers - can also check this on acqknolwedge \n",
    "# EDA, CORR, ECG, Feedback Cable - CBLCFMA - Current Feed\tStim - Custom, AMI / HLT - A16\tDigital (STP Input 0)\tDigital (STP Input 1)\tDigital (STP Input 2)\tDigital (STP Input 3)\tDigital (STP Input 4)\tDigital (STP Input 5)\tDigital (STP Input 6)\tDigital (STP Input 7)\n",
    "data.columns = ['EDA', 'Corr', 'ECGmV', 'feedback', 'stim', 'ch0', 'ch1', 'ch2', 'ch3', 'ch4', 'ch5', 'ch6', 'ch7']\n",
    "                 \n",
    "# first row has the header in it (1,2,3..13) need to remove it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba0ac3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum values for each column:\n",
      "EDA        -0.140568\n",
      "Corr       -0.181580\n",
      "ECGmV      -2.450867\n",
      "feedback   -7.290649\n",
      "stim       -0.217891\n",
      "ch0         0.000000\n",
      "ch1         0.000000\n",
      "ch2         0.000000\n",
      "ch3         0.000000\n",
      "ch4         0.000000\n",
      "ch5         0.000000\n",
      "ch6         0.000000\n",
      "ch7         0.000000\n",
      "dtype: float64\n",
      "\n",
      "Maximum values for each column:\n",
      "EDA          4.356198\n",
      "Corr         0.204468\n",
      "ECGmV        4.007874\n",
      "feedback    13.696289\n",
      "stim         6.630727\n",
      "ch0          5.000000\n",
      "ch1          5.000000\n",
      "ch2          5.000000\n",
      "ch3          5.000000\n",
      "ch4          5.000000\n",
      "ch5          5.000000\n",
      "ch6          5.000000\n",
      "ch7          5.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = data.iloc[1:].reset_index(drop=True)\n",
    "data\n",
    "min_values = data.min()\n",
    "max_values = data.max()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adb00080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Start_li \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[::\u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m      2\u001b[0m Start_num \u001b[38;5;241m=\u001b[39m find_non_none_indices (Start_li)\n\u001b[1;32m      3\u001b[0m startcode\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(Start_num))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'start'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c7bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6805ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efeaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5937631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
